{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf2f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Authenticated successfully\n",
      "âœ“ BigQuery client created\n",
      "âœ“ Project ID: 1044108980210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/cloud/bigquery/__init__.py:237: FutureWarning: %load_ext google.cloud.bigquery is deprecated. Install bigquery-magics package and use `%load_ext bigquery_magics`, instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BigQuery Setup & Authentication\n",
    "# =============================================================================\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pydata_google_auth\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Set your project ID (required for BigQuery queries)\n",
    "# Replace with your Google Cloud project ID\n",
    "PROJECT_ID = \"1044108980210\"  # <-- UPDATE THIS if needed\n",
    "\n",
    "# Authenticate with Google Cloud (opens browser for login)\n",
    "credentials = pydata_google_auth.get_user_credentials(\n",
    "    ['https://www.googleapis.com/auth/bigquery'],\n",
    "    use_local_webserver=True\n",
    ")\n",
    "\n",
    "# Create BigQuery client with credentials\n",
    "client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
    "\n",
    "# Load the BigQuery magic extension\n",
    "%load_ext google.cloud.bigquery\n",
    "\n",
    "print(f\"âœ“ Authenticated successfully\")\n",
    "print(f\"âœ“ BigQuery client created\")\n",
    "print(f\"âœ“ Project ID: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea28f2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ In-BigQuery pipeline functions loaded\n",
      "âœ“ Dataset: arbitrum_vlad_query\n",
      "âœ“ drop_all_analysis_tables() available for data reset\n",
      "âœ“ Now using bigquery-public-data.goog_blockchain_arbitrum_one_us.logs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FULLY IN-BIGQUERY Pipeline: No Local Upload Required\n",
    "# =============================================================================\n",
    "# This approach keeps EVERYTHING in BigQuery:\n",
    "# 1. Query attacks directly INTO a BigQuery table (not local)\n",
    "# 2. Join attacks table with token_transfers (both in BigQuery)\n",
    "# 3. Download only the final results\n",
    "#\n",
    "# MUCH faster for large datasets - no upload bottleneck!\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Dataset for storing our tables - ARBITRUM SPECIFIC\n",
    "BQ_DATASET = \"arbitrum_vlad_query\"\n",
    "\n",
    "def setup_bigquery_dataset():\n",
    "    \"\"\"Create the BigQuery dataset if it doesn't exist.\"\"\"\n",
    "    dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
    "    try:\n",
    "        client.get_dataset(dataset_id)\n",
    "        print(f\"âœ“ Dataset exists: {dataset_id}\")\n",
    "    except:\n",
    "        print(f\"ðŸ“ Creating dataset: {dataset_id}\")\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        client.create_dataset(dataset, exists_ok=True)\n",
    "        print(f\"âœ“ Created dataset: {dataset_id}\")\n",
    "    return dataset_id\n",
    "\n",
    "def drop_all_analysis_tables():\n",
    "    \"\"\"\n",
    "    âš ï¸  WARNING: This function PERMANENTLY DELETES all analysis tables in the dataset.\n",
    "    \n",
    "    Drops all tables used in the Arbitrum poisoning detection analysis:\n",
    "    - identified_attacks\n",
    "    - verified_attacks\n",
    "    - successful_attacks\n",
    "    \n",
    "    This allows you to recollect data with updated logic or parameters.\n",
    "    \n",
    "    âš ï¸  THIS ACTION CANNOT BE UNDONE - All data in these tables will be lost!\n",
    "    \n",
    "    Use this when you've made changes to the detection logic and need fresh results.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âš ï¸  WARNING: PERMANENT DATA DELETION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"This will DELETE all analysis tables in: {dataset_id}\")\n",
    "    print(\"\\nTables that will be dropped:\")\n",
    "    print(\"  - identified_attacks\")\n",
    "    print(\"  - verified_attacks\")\n",
    "    print(\"  - successful_attacks\")\n",
    "    print(\"\\nâš ï¸  THIS ACTION CANNOT BE UNDONE - All data will be permanently deleted!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Prompt user for confirmation\n",
    "    confirmation = input(\"\\nType 'DELETE_ALL_TABLES' to confirm, or press Enter to cancel: \").strip()\n",
    "    \n",
    "    if confirmation != \"DELETE_ALL_TABLES\":\n",
    "        print(\"\\nâœ— Deletion cancelled. No tables were deleted.\")\n",
    "        return\n",
    "    \n",
    "    # List of tables to drop\n",
    "    tables_to_drop = [\n",
    "        \"identified_attacks\",\n",
    "        \"verified_attacks\", \n",
    "        \"successful_attacks\"\n",
    "    ]\n",
    "    \n",
    "    dropped_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DROPPING TABLES...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for table_name in tables_to_drop:\n",
    "        try:\n",
    "            table_id = f\"{dataset_id}.{table_name}\"\n",
    "            client.delete_table(table_id, not_found_ok=True)\n",
    "            print(f\"âœ“ Dropped: {table_name}\")\n",
    "            dropped_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to drop {table_name}: {e}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DELETION COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"âœ“ Successfully dropped: {dropped_count} table(s)\")\n",
    "    if failed_count > 0:\n",
    "        print(f\"âœ— Failed to drop: {failed_count} table(s)\")\n",
    "    print(f\"\\nYou can now re-run the pipeline to recollect data with updated logic.\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ“ In-BigQuery pipeline functions loaded\")\n",
    "print(f\"âœ“ Dataset: {BQ_DATASET}\")\n",
    "print(\"âœ“ drop_all_analysis_tables() available for data reset\")\n",
    "print(\"âœ“ Now using bigquery-public-data.goog_blockchain_arbitrum_one_us.logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf16b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ query_verified_attacks_to_bigquery() loaded\n",
      "  - Data source: bigquery-public-data.goog_blockchain_arbitrum_one_us.logs\n",
      "  - Only flags attacks where BOTH prefix AND suffix match\n",
      "  - Uses block-based lookback (~20 minutes = 4600 blocks)\n",
      "  - Considers ALL victim transfers as potential intended addresses\n",
      "  - Deduplicates: Closest temporal match (most recent before attack)\n",
      "  - Filters out perfect matches (full 40-char address matches)\n",
      "  - Includes intended_address in output\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Final Arbitrum Query\n",
    "# =============================================================================\n",
    "# This version only identifies TRUE poisoning attacks where:\n",
    "# 1. Attacker address matches BOTH prefix AND suffix of a legitimate address victim used\n",
    "# 2. Captures the \"intended address\" being impersonated\n",
    "# 3. Filters out perfect matches (where attacker == intended address entirely)\n",
    "# Uses logs table for Arbitrum.\n",
    "\n",
    "def query_verified_attacks_to_bigquery(\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    table_name: str = \"verified_attacks\",\n",
    "    append: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Query attacks with prefix/suffix validation and intended address.\n",
    "    Only flags transfers as attacks if attacker address matches \n",
    "    BOTH the prefix AND suffix of an address the victim previously sent to.\n",
    "    Uses logs table for Arbitrum.\n",
    "    \n",
    "    Lookback is in BLOCKS (4600 blocks â‰ˆ 20 minutes on Arbitrum).\n",
    "    min_victim_transfer_usd: Minimum value of victim's prior transfer to count as \"intended address\".\n",
    "                             Set to 0 to consider ALL victim transfers as potential intended addresses.\n",
    "    \n",
    "    Deduplication Strategy: When multiple historical transfers match the same attack,\n",
    "    keeps the transfer that is CLOSEST in time (temporal proximity) to the attack,\n",
    "    as this is most likely the address the attacker observed the victim sending to.\n",
    "    \n",
    "    Perfect Match Filtering: Removes any cases where attacker_address == intended_address\n",
    "    for all 40 hex characters, as these are not poisoning attacks but legitimate transfers.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"VERIFIED ATTACK DETECTION (with prefix/suffix matching)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Date range: {start_date} to {end_date}\")\n",
    "    print(f\"Deduplication: Closest temporal match (most recent before attack)\")\n",
    "    print(f\"Filters: Removes perfect matches (full 40-char address match)\")\n",
    "    print(f\"Data source: Arbitrum logs table\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    setup_bigquery_dataset()\n",
    "    dest_table = f\"{PROJECT_ID}.{BQ_DATASET}.{table_name}\"\n",
    "    \n",
    "    # For the date filter, use a larger buffer to ensure we capture victim history\n",
    "    # at month boundaries. Use 7 days to ensure we capture previous month's data\n",
    "    # even if attacks happen on the first day of a new month.\n",
    "    lookback_date = (pd.to_datetime(start_date) - pd.Timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    -- 1. CONFIGURATION\n",
    "    DECLARE analysis_start TIMESTAMP DEFAULT TIMESTAMP(\"{start_date}\");\n",
    "    DECLARE analysis_end   TIMESTAMP DEFAULT TIMESTAMP(\"{end_date}\");\n",
    "\n",
    "    -- Logic Settings\n",
    "    DECLARE lookahead_blocks INT64 DEFAULT 4600; \n",
    "    DECLARE prefix_len INT64 DEFAULT 3;\n",
    "    DECLARE suffix_len INT64 DEFAULT 4;\n",
    "    DECLARE dust_threshold FLOAT64 DEFAULT 1.0; -- Upper limit for a \"Tiny\" transfer ($1.00)\n",
    "\n",
    "    DECLARE start_block DEFAULT (\n",
    "    SELECT MIN(block_number)\n",
    "    FROM `bigquery-public-data.goog_blockchain_arbitrum_one_us.logs`\n",
    "    WHERE block_timestamp >= analysis_start\n",
    "    );\n",
    "\n",
    "    DECLARE end_block DEFAULT (\n",
    "    SELECT MAX(block_number)\n",
    "    FROM `bigquery-public-data.goog_blockchain_arbitrum_one_us.logs`\n",
    "    WHERE block_timestamp < analysis_end\n",
    "    );\n",
    "\n",
    "    CREATE TEMP FUNCTION HexToBigNumeric(hex STRING)\n",
    "    RETURNS BIGNUMERIC\n",
    "    LANGUAGE js AS \\\"\\\"\\\"\n",
    "    // Remove 0x prefix if present\n",
    "    hex = hex.startsWith(\"0x\") ? hex.slice(2) : hex;\n",
    "\n",
    "    // Convert hex to BigInt\n",
    "    const value = BigInt('0x' + hex);\n",
    "\n",
    "    // Return as string so BigQuery can cast it\n",
    "    return value.toString();\n",
    "    \\\"\\\"\\\";\n",
    "\n",
    "    -- 2. CREATE OR INSERT INTO TABLE\n",
    "    {'CREATE OR REPLACE TABLE `' + dest_table + '` AS' if not append else 'INSERT INTO `' + dest_table + '`'}\n",
    "\n",
    "    -- 3. DATA EXTRACTION\n",
    "    WITH raw_transfers AS (\n",
    "        SELECT\n",
    "        block_number,\n",
    "        block_timestamp,\n",
    "        transaction_hash,\n",
    "        address AS token_address,\n",
    "        -- Decode from & to (topics[1] and topics[2])\n",
    "        CONCAT(\"0x\", SUBSTR(topics[SAFE_OFFSET(1)], 27)) AS from_addr,\n",
    "        CONCAT(\"0x\", SUBSTR(topics[SAFE_OFFSET(2)], 27)) AS to_addr,\n",
    "        -- cast from amounts\n",
    "        HexToBigNumeric(data) AS value,\n",
    "        HexToBigNumeric(data) / 1e6 AS value_usd,\n",
    "        -- add token symbol manually\n",
    "        -- CASE \n",
    "        --     WHEN LOWER(address) = usdc_native  THEN \"USDC.e\"\n",
    "        --     WHEN LOWER(address) = usdc_bridged THEN \"USDC\"\n",
    "        --     WHEN LOWER(address) = usdt_addr    THEN \"USDT\"\n",
    "        -- END AS token_symbol\n",
    "        FROM `bigquery-public-data.goog_blockchain_arbitrum_one_us.logs`\n",
    "        WHERE block_number BETWEEN start_block AND end_block\n",
    "        AND removed = FALSE\n",
    "        AND topics[SAFE_OFFSET(0)] = \"0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef\"\n",
    "        AND address IN (\n",
    "            \"0xaf88d065e77c8cc2239327c5edb3a432268e5831\", -- USDC Native\n",
    "            \"0xff970a61a04b1ca14834a43f5de4533ebddb5cc8\", -- USDC Bridged\n",
    "            \"0xfd086bc7cd5c481dcc9c85ebe478a1c0b69fcbb9\"  -- USDT\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    -- 4. IDENTIFY PAIRS\n",
    "    attack_pairs AS (\n",
    "        SELECT\n",
    "            S.transaction_hash AS attack_tx_hash,\n",
    "            S.block_number AS attack_block,\n",
    "            S.block_timestamp AS attack_ts,\n",
    "            -- S.token_symbol,\n",
    "            S.value_usd,\n",
    "            \n",
    "            -- NEW COLUMN: Attack Type\n",
    "            CASE \n",
    "                WHEN S.value_usd = 0 THEN 'zero_value'\n",
    "                ELSE 'tiny_transfer'\n",
    "            END AS attack_type,\n",
    "\n",
    "            -- Attacker Labeling\n",
    "            CASE \n",
    "                WHEN S.from_addr = V.from_addr THEN S.to_addr \n",
    "                ELSE S.from_addr \n",
    "            END AS attacker_address,\n",
    "\n",
    "            V.from_addr AS victim_address,\n",
    "            V.to_addr AS intended_address,\n",
    "            V.transaction_hash AS intended_tx_hash,\n",
    "            V.block_timestamp AS intended_ts,\n",
    "            V.block_number AS intended_block,\n",
    "            \n",
    "            CASE WHEN S.from_addr = V.from_addr THEN 'spoof_outgoing' ELSE 'spam_incoming' END AS direction\n",
    "\n",
    "        FROM raw_transfers AS V -- History\n",
    "        JOIN raw_transfers AS S -- Attack\n",
    "            ON S.token_address = V.token_address\n",
    "            AND S.block_number >= V.block_number\n",
    "            AND S.block_number <= (V.block_number + lookahead_blocks)\n",
    "        WHERE\n",
    "            V.value_usd > 0\n",
    "            AND V.block_timestamp BETWEEN analysis_start AND analysis_end\n",
    "            \n",
    "            -- MODIFIED FILTER: Allow 0 OR Small Values\n",
    "            AND S.value_usd <= dust_threshold\n",
    "            \n",
    "            AND (\n",
    "                (S.from_addr = V.from_addr)\n",
    "                OR\n",
    "                (S.to_addr = V.from_addr)\n",
    "            )\n",
    "            AND S.transaction_hash != V.transaction_hash\n",
    "    )\n",
    "\n",
    "    -- 5. FINAL SELECTION\n",
    "    SELECT \n",
    "        attack_tx_hash,\n",
    "        attack_block,\n",
    "        attack_ts,\n",
    "        -- token_symbol,\n",
    "        attack_type, -- <--- Included in final output\n",
    "        value_usd,   -- <--- Good to verify the actual amount\n",
    "        direction,\n",
    "        attacker_address,\n",
    "        victim_address,\n",
    "        intended_address,\n",
    "        intended_ts,\n",
    "        (attack_block - intended_block) AS blocks_delay\n",
    "    FROM attack_pairs\n",
    "    WHERE\n",
    "        SUBSTR(attacker_address, 3, prefix_len) = SUBSTR(intended_address, 3, prefix_len)\n",
    "        AND\n",
    "        SUBSTR(attacker_address, -1 * suffix_len) = SUBSTR(intended_address, -1 * suffix_len)\n",
    "        AND attacker_address != intended_address\n",
    "\n",
    "    -- DEDUPLICATION\n",
    "    QUALIFY ROW_NUMBER() OVER(\n",
    "        PARTITION BY attack_tx_hash, attacker_address, victim_address, direction\n",
    "        ORDER BY intended_ts DESC\n",
    "    ) = 1\n",
    "    ORDER BY attack_ts, attack_tx_hash\n",
    "    \"\"\"\n",
    "    \n",
    "    # DON'T set destination in job_config - the SQL script handles it\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    print(f\"\\nðŸ” Running verified attack detection...\")\n",
    "    print(f\"ðŸ“ Destination: {dest_table}\")\n",
    "    \n",
    "    job = client.query(query, job_config=job_config)\n",
    "    job.result()\n",
    "    \n",
    "    table = client.get_table(dest_table)\n",
    "    print(f\"\\nâœ“ Query complete!\")\n",
    "    print(f\"âœ“ Verified attacks found: {table.num_rows:,}\")\n",
    "    print(f\"âœ“ Table: {dest_table}\")\n",
    "    \n",
    "    return dest_table\n",
    "\n",
    "print(\"âœ“ query_verified_attacks_to_bigquery() loaded\")\n",
    "print(\"  - Data source: bigquery-public-data.goog_blockchain_arbitrum_one_us.logs\")\n",
    "print(\"  - Only flags attacks where BOTH prefix AND suffix match\")\n",
    "print(\"  - Uses block-based lookback (~20 minutes = 4600 blocks)\")\n",
    "print(\"  - Considers ALL victim transfers as potential intended addresses\")\n",
    "print(\"  - Deduplicates: Closest temporal match (most recent before attack)\")\n",
    "print(\"  - Filters out perfect matches (full 40-char address matches)\")\n",
    "print(\"  - Includes intended_address in output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1194182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BATCH PIPELINE: Verified Address Poisoning Detection (PARALLEL)\n",
      "======================================================================\n",
      "Date range: 2025-01-01 to 2025-02-01\n",
      "Min victim transfer: $0.0 (0 = all transfers)\n",
      "Chunk size: 1 day(s)\n",
      "Max concurrent queries: 10\n",
      "Mode: CREATE new table\n",
      "======================================================================\n",
      "\n",
      "Total chunks to process: 31\n",
      "  1. 2025-01-01 to 2025-01-02\n",
      "  2. 2025-01-02 to 2025-01-03\n",
      "  3. 2025-01-03 to 2025-01-04\n",
      "  4. 2025-01-04 to 2025-01-05\n",
      "  5. 2025-01-05 to 2025-01-06\n",
      "  ... and 26 more chunks\n",
      "\n",
      "======================================================================\n",
      "PROCESSING CHUNKS IN PARALLEL...\n",
      "======================================================================\n",
      "\n",
      "ðŸ”§ Creating table with first chunk...\n",
      "\n",
      "--- Chunk 1/31: 2025-01-01 to 2025-01-02 ---\n",
      "============================================================\n",
      "VERIFIED ATTACK DETECTION (with prefix/suffix matching)\n",
      "============================================================\n",
      "Date range: 2025-01-01 to 2025-01-02\n",
      "Deduplication: Closest temporal match (most recent before attack)\n",
      "Filters: Removes perfect matches (full 40-char address match)\n",
      "Data source: Arbitrum logs table\n",
      "============================================================\n",
      "ðŸ“ Creating dataset: 1044108980210.arbitrum_vlad_query\n",
      "âœ“ Created dataset: 1044108980210.arbitrum_vlad_query\n",
      "\n",
      "ðŸ” Running verified attack detection...\n",
      "ðŸ“ Destination: 1044108980210.arbitrum_vlad_query.arbitrum_attacks\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FULL PIPELINE: Verified Address Poisoning Detection (Batch by Time Period)\n",
    "# =============================================================================\n",
    "# This pipeline detects TRUE address poisoning attacks using prefix/suffix matching.\n",
    "# Processes data in configurable time chunks to avoid timeouts and manage costs.\n",
    "#\n",
    "# All results are appended to a single BigQuery table.\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def get_month_ranges(year: int) -> list:\n",
    "    \"\"\"\n",
    "    Generate list of (start_date, end_date) tuples for each month in a year.\n",
    "    \n",
    "    Args:\n",
    "        year: The year to generate months for (e.g., 2023)\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: [(\"2023-01-01\", \"2023-02-01\"), (\"2023-02-01\", \"2023-03-01\"), ...]\n",
    "    \"\"\"\n",
    "    months = []\n",
    "    for month in range(1, 13):\n",
    "        start = f\"{year}-{month:02d}-01\"\n",
    "        if month == 12:\n",
    "            end = f\"{year + 1}-01-01\"\n",
    "        else:\n",
    "            end = f\"{year}-{month + 1:02d}-01\"\n",
    "        months.append((start, end))\n",
    "    return months\n",
    "\n",
    "\n",
    "def get_month_ranges_between(start_date: str, end_date: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate list of (start_date, end_date) tuples for each month between two dates.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date 'YYYY-MM-DD'\n",
    "        end_date: End date 'YYYY-MM-DD' (exclusive)\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples for each month in the range\n",
    "    \"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    months = []\n",
    "    current = start.replace(day=1)\n",
    "    \n",
    "    while current < end:\n",
    "        month_start = max(current, start)\n",
    "        if current.month == 12:\n",
    "            next_month = current.replace(year=current.year + 1, month=1)\n",
    "        else:\n",
    "            next_month = current.replace(month=current.month + 1)\n",
    "        month_end = min(next_month, end)\n",
    "        \n",
    "        months.append((month_start.strftime(\"%Y-%m-%d\"), month_end.strftime(\"%Y-%m-%d\")))\n",
    "        current = next_month\n",
    "    \n",
    "    return months\n",
    "\n",
    "\n",
    "def get_day_ranges_between(start_date: str, end_date: str, chunk_days: int = 3) -> list:\n",
    "    \"\"\"\n",
    "    Generate list of (start_date, end_date) tuples for each N-day chunk between two dates.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date 'YYYY-MM-DD'\n",
    "        end_date: End date 'YYYY-MM-DD' (exclusive)\n",
    "        chunk_days: Number of days per chunk (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples for each N-day period in the range\n",
    "    \"\"\"\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    chunks = []\n",
    "    current = start\n",
    "    \n",
    "    while current < end:\n",
    "        chunk_start = current\n",
    "        chunk_end = min(current + timedelta(days=chunk_days), end)\n",
    "        \n",
    "        chunks.append((chunk_start.strftime(\"%Y-%m-%d\"), chunk_end.strftime(\"%Y-%m-%d\")))\n",
    "        current = chunk_end\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_chunk(chunk_start: str, chunk_end: str, table_name: str, chunk_idx: int, total_chunks: int, append: bool):\n",
    "    \"\"\"\n",
    "    Process a single chunk of data.\n",
    "    \n",
    "    Args:\n",
    "        chunk_start: Start date for this chunk\n",
    "        chunk_end: End date for this chunk\n",
    "        table_name: Name of the BigQuery table\n",
    "        chunk_idx: Index of this chunk (0-based)\n",
    "        total_chunks: Total number of chunks\n",
    "        append: Whether to append or create/replace table\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (chunk_idx, success, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n--- Chunk {chunk_idx + 1}/{total_chunks}: {chunk_start} to {chunk_end} ---\")\n",
    "        \n",
    "        query_verified_attacks_to_bigquery(\n",
    "            start_date=chunk_start,\n",
    "            end_date=chunk_end,\n",
    "            table_name=table_name,\n",
    "            append=append\n",
    "        )\n",
    "        \n",
    "        return (chunk_idx, True, None)\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"  âŒ ERROR in chunk {chunk_idx + 1}: {error_msg}\")\n",
    "        return (chunk_idx, False, error_msg)\n",
    "\n",
    "\n",
    "def run_batch_pipeline(\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    table_name: str = \"verified_attacks\",\n",
    "    min_victim_transfer_usd: float = 0.0,\n",
    "    chunk_days: int = 3,\n",
    "    max_concurrent: int = 5,\n",
    "    skip_table_creation: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the verified attack detection pipeline in time-based batches with parallel execution.\n",
    "    All results are appended to a single BigQuery table.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date 'YYYY-MM-DD'\n",
    "        end_date: End date 'YYYY-MM-DD' (exclusive)\n",
    "        table_name: Name for the BigQuery results table\n",
    "        min_victim_transfer_usd: Minimum value of victim's prior transfer (0 = all transfers considered)\n",
    "        chunk_days: Number of days per chunk (default: 3)\n",
    "        max_concurrent: Maximum number of concurrent queries (default: 5, max recommended: 10)\n",
    "        skip_table_creation: If True, append to existing table instead of creating new one (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        Full BigQuery table ID\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BATCH PIPELINE: Verified Address Poisoning Detection (PARALLEL)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Date range: {start_date} to {end_date}\")\n",
    "    print(f\"Min victim transfer: ${min_victim_transfer_usd} (0 = all transfers)\")\n",
    "    print(f\"Chunk size: {chunk_days} day(s)\")\n",
    "    print(f\"Max concurrent queries: {max_concurrent}\")\n",
    "    print(f\"Mode: {'APPEND to existing table' if skip_table_creation else 'CREATE new table'}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    chunks = get_day_ranges_between(start_date, end_date, chunk_days=chunk_days)\n",
    "    print(f\"\\nTotal chunks to process: {len(chunks)}\")\n",
    "    for i, (s, e) in enumerate(chunks[:5]):  # Show first 5 chunks\n",
    "        print(f\"  {i+1}. {s} to {e}\")\n",
    "    if len(chunks) > 5:\n",
    "        print(f\"  ... and {len(chunks) - 5} more chunks\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PROCESSING CHUNKS IN PARALLEL...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    dest_table = f\"{PROJECT_ID}.{BQ_DATASET}.{table_name}\"\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    errors = []\n",
    "    \n",
    "    if skip_table_creation:\n",
    "        # All chunks run in parallel with append=True\n",
    "        print(f\"\\nðŸš€ Appending to existing table with up to {max_concurrent} concurrent queries...\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:\n",
    "            # Submit all chunks\n",
    "            futures = {\n",
    "                executor.submit(\n",
    "                    process_chunk, \n",
    "                    chunk_start, \n",
    "                    chunk_end, \n",
    "                    table_name, \n",
    "                    i,\n",
    "                    len(chunks),\n",
    "                    append=True  # All chunks append\n",
    "                ): (i, chunk_start, chunk_end)\n",
    "                for i, (chunk_start, chunk_end) in enumerate(chunks)\n",
    "            }\n",
    "            \n",
    "            # Process completed chunks\n",
    "            for future in tqdm(as_completed(futures), total=len(chunks), desc=\"Chunks\"):\n",
    "                chunk_idx, chunk_start, chunk_end = futures[future]\n",
    "                idx, success, error_msg = future.result()\n",
    "                \n",
    "                if success:\n",
    "                    successful += 1\n",
    "                else:\n",
    "                    failed += 1\n",
    "                    errors.append((chunk_start, chunk_end, error_msg))\n",
    "    else:\n",
    "        # Process first chunk synchronously to create the table\n",
    "        if len(chunks) > 0:\n",
    "            print(\"\\nðŸ”§ Creating table with first chunk...\")\n",
    "            first_start, first_end = chunks[0]\n",
    "            result = process_chunk(first_start, first_end, table_name, 0, len(chunks), append=False)\n",
    "            \n",
    "            if not result[1]:\n",
    "                print(f\"\\nâŒ Failed to create table with first chunk. Aborting.\")\n",
    "                print(f\"Error: {result[2]}\")\n",
    "                return dest_table\n",
    "            \n",
    "            print(f\"âœ“ Table created successfully\")\n",
    "            successful = 1\n",
    "        \n",
    "        # Process remaining chunks in parallel\n",
    "        remaining_chunks = chunks[1:]\n",
    "        \n",
    "        if len(remaining_chunks) > 0:\n",
    "            print(f\"\\nðŸš€ Processing {len(remaining_chunks)} remaining chunks with up to {max_concurrent} concurrent queries...\")\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=max_concurrent) as executor:\n",
    "                # Submit all chunks\n",
    "                futures = {\n",
    "                    executor.submit(\n",
    "                        process_chunk, \n",
    "                        chunk_start, \n",
    "                        chunk_end, \n",
    "                        table_name, \n",
    "                        i + 1,  # +1 because we already processed chunk 0\n",
    "                        len(chunks),\n",
    "                        append=True  # Remaining chunks append\n",
    "                    ): (i + 1, chunk_start, chunk_end)\n",
    "                    for i, (chunk_start, chunk_end) in enumerate(remaining_chunks)\n",
    "                }\n",
    "                \n",
    "                # Process completed chunks\n",
    "                for future in tqdm(as_completed(futures), total=len(remaining_chunks), desc=\"Chunks\"):\n",
    "                    chunk_idx, chunk_start, chunk_end = futures[future]\n",
    "                    idx, success, error_msg = future.result()\n",
    "                    \n",
    "                    if success:\n",
    "                        successful += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                        errors.append((chunk_start, chunk_end, error_msg))\n",
    "    \n",
    "    # Get final count\n",
    "    try:\n",
    "        count = client.query(f\"SELECT COUNT(*) as cnt FROM `{dest_table}`\").to_dataframe()['cnt'].iloc[0]\n",
    "        total_attacks = count\n",
    "    except:\n",
    "        total_attacks = \"Unknown\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BATCH PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"âœ“ Successful chunks: {successful}/{len(chunks)}\")\n",
    "    if failed > 0:\n",
    "        print(f\"âŒ Failed chunks: {failed}/{len(chunks)}\")\n",
    "        print(f\"\\nFailed chunks:\")\n",
    "        for start, end, error in errors[:10]:  # Show first 10 errors\n",
    "            print(f\"  - {start} to {end}: {error[:100]}\")\n",
    "        if len(errors) > 10:\n",
    "            print(f\"  ... and {len(errors) - 10} more errors\")\n",
    "    print(f\"âœ“ Total verified attacks: {total_attacks:,}\" if isinstance(total_attacks, int) else f\"âœ“ Total verified attacks: {total_attacks}\")\n",
    "    print(f\"âœ“ BigQuery table: {dest_table}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return dest_table\n",
    "\n",
    "\n",
    "# ----------------------------- CONFIGURATION ---------------------------------\n",
    "PIPELINE_START_DATE = \"2025-01-01\"\n",
    "PIPELINE_END_DATE = \"2025-02-01\"  # Exclusive\n",
    "TABLE_NAME = \"arbitrum_attacks\"\n",
    "CHUNK_DAYS = 1  # Process data in 3-day chunks\n",
    "MAX_CONCURRENT = 10  # Maximum concurrent queries (5-10 recommended to avoid rate limits)\n",
    "SKIP_TABLE_CREATION = False  # Set to True to append to existing table instead of creating new\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Run the batch pipeline\n",
    "verified_attacks_table = run_batch_pipeline(\n",
    "    start_date=PIPELINE_START_DATE,\n",
    "    end_date=PIPELINE_END_DATE,\n",
    "    table_name=TABLE_NAME,\n",
    "    chunk_days=CHUNK_DAYS,\n",
    "    max_concurrent=MAX_CONCURRENT,\n",
    "    skip_table_creation=SKIP_TABLE_CREATION\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
